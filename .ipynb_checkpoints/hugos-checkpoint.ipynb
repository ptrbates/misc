{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yeah, the Hugo Awards.\n",
    "\n",
    "I think I've consistently liked these books. I'm about to need another one. Will one of these books be the book of my dreams? What books that I've already read have won a Hugo and I didn't EVEN KNOW IT!?!?!!!\n",
    "\n",
    "Are there certain authors who show up more than others? \n",
    "\n",
    "What else can we figure out about the books that receive awards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, let's get links to the years that awards have been given out.\n",
    "url = \"http://www.thehugoawards.org/hugo-history/\"\n",
    "\n",
    "# Save the html from that page\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "# Collect the links\n",
    "links = []\n",
    "for link in soup.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "    \n",
    "pattern = re.compile('http://www.thehugoawards.org/hugo-history/\\d+')\n",
    "pages = []\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        if bool(pattern.search(link)):\n",
    "                pages.append(link)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "pages = sorted(list(set(pages)), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Hugo Awards website doesn't have consistent formatting across all the pages, so the extracting of the finalist novels for each year isn't completely straightforward. Most of the pages follow a particular pattern; I make exceptions for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to retrieve the finalist novels for a given year\n",
    "\n",
    "def get_novels(link):\n",
    "    # Get the html\n",
    "    r = requests.get(link)\n",
    "    text = r.text\n",
    "    pagesoup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Get the year\n",
    "    year = pagesoup.find_all('title')[0].text[0:4]\n",
    "\n",
    "    # Get the finalist books and authors        \n",
    "    pagetext = pagesoup.get_text()\n",
    "\n",
    "    if year == '2004':\n",
    "        # In 2004 (entry #13), we have to adjust the block that's retrieved, \n",
    "        # since my locating term is used in an unexpected place in the page text\n",
    "        novels = pagetext.split('Best ')[2]\n",
    "    elif year == '2017':\n",
    "        # In 2017 (entry #0), something similar happens. I can also adjust this easily. \n",
    "        # If this happened more often, I would adjust my strategy; for only two unusual cases, I'll leave it as-is.\n",
    "        novels = pagetext.split('Best ')[3]\n",
    "    else:\n",
    "        # It turns out that Hugos were only given to periodicals in 1957, \n",
    "        # so no novels are expected in big_list. This accounts for entry #60. The other years all work.\n",
    "        novels = pagetext.split('Best ')[1]\n",
    "    \n",
    "    novels = novels.split('\\n')\n",
    "    \n",
    "    # Collect the books into a list to return\n",
    "    books = []\n",
    "    pattern1 = re.compile(r'[A-Z]')\n",
    "    pattern2 = re.compile(r'(, )|( by )')\n",
    "    pattern3 = re.compile(r'(\\(.*\\))|(\\[.*\\])')\n",
    "    \n",
    "    for novel in novels:\n",
    "        try:\n",
    "            if bool(pattern2.search(novel)) & bool(pattern3.search(novel)):\n",
    "                if not bool(re.compile(r'^Novel').match(novel)):\n",
    "                    books.append(novel)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return({year: books})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see whether hugofinalists.p already exists\n",
    "if os.path.exists('hugofinalists.p'):\n",
    "    # If it does, go ahead and load it into finalists\n",
    "    with open('hugofinalists.p', 'rb') as f:\n",
    "        finalists = pickle.load(f)\n",
    "\n",
    "# If it doesn't, build a list of the finalists and dump it to the file for next time\n",
    "else:\n",
    "    finalists = []\n",
    "    for link in pages:\n",
    "        time.sleep(1)\n",
    "        finalists.append(get_novels(link))\n",
    "            \n",
    "    with open('hugofinalists.p', 'wb') as f:\n",
    "        pickle.dump(finalists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the novel information into bits: title, author, publishing information\n",
    "pattern1 = re.compile(r', by ')\n",
    "pattern2 = re.compile(r' by ')\n",
    "pattern3 = re.compile(r', ')\n",
    "pattern4 = re.compile(r' \\[')\n",
    "pattern5 = re.compile(r' \\(')\n",
    "listoflists = []\n",
    "\n",
    "for d in finalists:\n",
    "    for key, value in d.items():\n",
    "        for item in value:\n",
    "            if bool(pattern1.search(item)):\n",
    "                split1 = pattern1.split(item, maxsplit=1)\n",
    "            elif bool(pattern2.search(item)):\n",
    "                split1 = pattern2.split(item, maxsplit=1)\n",
    "            elif bool(pattern3.search(item)):\n",
    "                split1 = pattern3.split(item, maxsplit=1)\n",
    "            if bool(pattern4.search(split1[1])):\n",
    "                split2 = split1[1].rsplit(r' [', maxsplit=1)\n",
    "            elif bool(pattern5.search(split1[1])):\n",
    "                split2 = split1[1].rsplit(r' (', maxsplit=1)\n",
    "            listoflists.append([key, split1[0], split2[0], split2[1].replace(')', '').replace(']', '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataframe of the final results\n",
    "hugos = pd.DataFrame(listoflists, columns=['year', 'title', 'author', 'published'])\n",
    "\n",
    "# Some of the names can be cleaned up a little; make formatting (mostly) consistent\n",
    "hugos.author = [author.lower().replace(r\". \", r\".\").replace(r\".\", r\". \").title() for author in hugos.author]\n",
    "hugos.author[20] = 'Larry Correia'\n",
    "hugos.author[5] = 'Cixin Liu, translated by Ken Liu'\n",
    "hugos.author[11] = 'Cixin Liu, translated by Ken Liu'\n",
    "hugos.author[351] = 'Edward E. Smith'\n",
    "hugos.author[357] = 'Edward E. Smith'\n",
    "\n",
    "# Make the year column numeric\n",
    "hugos['year'] = pd.to_numeric(hugos['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe to csv\n",
    "hugos.to_csv('hugos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
